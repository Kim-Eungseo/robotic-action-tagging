"""
Hugging Face Libero ë°ì´í„°ì…‹ì„ Polars DataFrameìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import logging
import polars as pl
import numpy as np
import base64
import io
from PIL import Image
import datasets
from tqdm import tqdm
import json
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)


def image_to_base64_optimized(img_array, quality=75, max_size=512):
    """ì´ë¯¸ì§€ë¥¼ ìµœì í™”ëœ base64ë¡œ ë³€í™˜"""
    try:
        if isinstance(img_array, np.ndarray):
            if img_array.dtype != np.uint8:
                img_array = (img_array * 255).astype(np.uint8)
            img = Image.fromarray(img_array)
        else:
            img = img_array

        # í¬ê¸° ìµœì í™”
        if max_size and (img.width > max_size or img.height > max_size):
            img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)

        buffer = io.BytesIO()
        if img.mode == "RGBA":
            img = img.convert("RGB")
        img.save(buffer, format="JPEG", quality=quality, optimize=True)

        return base64.b64encode(buffer.getvalue()).decode()
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None


def convert_libero_to_parquet(output_dir="data", sample_size=None):
    """Libero ë°ì´í„°ì…‹ì„ Parquetìœ¼ë¡œ ë³€í™˜"""
    logger.info("ğŸ¤– Libero ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘...")

    try:
        # ë°ì´í„°ì…‹ ë¡œë“œ
        dataset = datasets.load_dataset("physical-intelligence/libero")
        train_data = dataset["train"]

        logger.info(f"ğŸ“Š ì´ ë°ì´í„° í¬ê¸°: {len(train_data)}")

        # ìƒ˜í”Œë§ (í…ŒìŠ¤íŠ¸ìš©)
        if sample_size:
            indices = list(range(0, len(train_data), len(train_data) // sample_size))
            train_data = train_data.select(indices)
            logger.info(f"ğŸ¯ ìƒ˜í”Œë§ëœ ë°ì´í„° í¬ê¸°: {len(train_data)}")

        # ë°ì´í„° ë³€í™˜
        logger.info("ğŸ”„ ë°ì´í„° ë³€í™˜ ì‹œì‘...")

        converted_data = []

        for i, row in enumerate(tqdm(train_data, desc="ë³€í™˜ ì¤‘")):
            try:
                # ì´ë¯¸ì§€ ë³€í™˜ (ë©”ì¸ ì¹´ë©”ë¼)
                main_image_b64 = image_to_base64_optimized(
                    row["image"], quality=75, max_size=512
                )

                # ì´ë¯¸ì§€ ë³€í™˜ (ì†ëª© ì¹´ë©”ë¼)
                wrist_image_b64 = image_to_base64_optimized(
                    row["wrist_image"], quality=70, max_size=256
                )

                if main_image_b64 is None or wrist_image_b64 is None:
                    logger.warning(f"âš ï¸  ì¸ë±ìŠ¤ {i} ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨, ìŠ¤í‚µ")
                    continue

                converted_row = {
                    "episode_index": int(row["episode_index"]),
                    "frame_index": int(row["frame_index"]),
                    "task_index": int(row["task_index"]),
                    "timestamp": float(row["timestamp"]),
                    "main_image": main_image_b64,
                    "wrist_image": wrist_image_b64,
                    "state": json.dumps(
                        [float(x) for x in row["state"]]
                    ),  # JSON ë¬¸ìì—´ë¡œ ì €ì¥
                    "actions": json.dumps(
                        [float(x) for x in row["actions"]]
                    ),  # JSON ë¬¸ìì—´ë¡œ ì €ì¥
                }

                converted_data.append(converted_row)

                # ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•´ ì¤‘ê°„ì— ì €ì¥
                if len(converted_data) >= 1000:
                    logger.info(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥ ({len(converted_data)} í•­ëª©)")
                    save_batch(converted_data, output_dir, i // 1000)
                    converted_data = []

            except Exception as e:
                logger.error(f"âŒ ì¸ë±ìŠ¤ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue

        # ë‚¨ì€ ë°ì´í„° ì €ì¥
        if converted_data:
            save_batch(converted_data, output_dir, "final")

        logger.info("âœ… ë°ì´í„° ë³€í™˜ ì™„ë£Œ!")

        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "total_frames": len(train_data),
            "total_episodes": len(set(row["episode_index"] for row in train_data)),
            "total_tasks": len(set(row["task_index"] for row in train_data)),
            "conversion_time": datetime.now().isoformat(),
            "sample_size": sample_size,
        }

        with open(os.path.join(output_dir, "metadata.json"), "w") as f:
            json.dump(metadata, f, indent=2)

        logger.info("ğŸ“‹ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        logger.error(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        raise


def save_batch(data, output_dir, batch_num):
    """ë°ì´í„° ë°°ì¹˜ë¥¼ Parquetìœ¼ë¡œ ì €ì¥"""
    try:
        os.makedirs(output_dir, exist_ok=True)

        # Polars DataFrame ìƒì„±
        df = pl.DataFrame(data)

        # Parquetìœ¼ë¡œ ì €ì¥
        output_path = os.path.join(output_dir, f"libero_batch_{batch_num}.parquet")
        df.write_parquet(output_path, compression="snappy")

        logger.info(f"âœ… ë°°ì¹˜ {batch_num} ì €ì¥ ì™„ë£Œ: {output_path}")

    except Exception as e:
        logger.error(f"âŒ ë°°ì¹˜ {batch_num} ì €ì¥ ì‹¤íŒ¨: {e}")
        raise


def merge_parquet_files(input_dir="data", output_file="data/libero_complete.parquet"):
    """ì—¬ëŸ¬ Parquet íŒŒì¼ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°"""
    try:
        logger.info("ğŸ”— Parquet íŒŒì¼ë“¤ í•©ì¹˜ëŠ” ì¤‘...")

        parquet_files = [
            f for f in os.listdir(input_dir) if f.endswith(".parquet") and "batch" in f
        ]

        if not parquet_files:
            logger.error("âŒ Parquet íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return

        # ì²« ë²ˆì§¸ íŒŒì¼ë¡œ ì‹œì‘
        df = pl.read_parquet(os.path.join(input_dir, parquet_files[0]))

        # ë‚˜ë¨¸ì§€ íŒŒì¼ë“¤ í•©ì¹˜ê¸°
        for file in tqdm(parquet_files[1:], desc="íŒŒì¼ í•©ì¹˜ëŠ” ì¤‘"):
            batch_df = pl.read_parquet(os.path.join(input_dir, file))
            df = pl.concat([df, batch_df])

        # ì •ë ¬ (episode_index, frame_index ìˆœ)
        df = df.sort(["episode_index", "frame_index"])

        # ìµœì¢… íŒŒì¼ ì €ì¥
        df.write_parquet(output_file, compression="snappy")

        logger.info(f"âœ… ìµœì¢… íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_file}")
        logger.info(f"ğŸ“Š ì´ í–‰ ìˆ˜: {len(df)}")

        return df

    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ í•©ì¹˜ê¸° ì‹¤íŒ¨: {e}")
        raise


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Libero ë°ì´í„°ì…‹ì„ Polars Parquetìœ¼ë¡œ ë³€í™˜"
    )
    parser.add_argument("--output-dir", default="data", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument(
        "--sample-size", type=int, help="ìƒ˜í”Œë§í•  ë°ì´í„° í¬ê¸° (í…ŒìŠ¤íŠ¸ìš©)"
    )
    parser.add_argument(
        "--merge-only", action="store_true", help="ê¸°ì¡´ ë°°ì¹˜ íŒŒì¼ë“¤ë§Œ í•©ì¹˜ê¸°"
    )

    args = parser.parse_args()

    if args.merge_only:
        merge_parquet_files(args.output_dir)
    else:
        # ë³€í™˜ ì‹¤í–‰
        convert_libero_to_parquet(args.output_dir, args.sample_size)

        # ìë™ìœ¼ë¡œ í•©ì¹˜ê¸°
        merge_parquet_files(args.output_dir)

    logger.info("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
